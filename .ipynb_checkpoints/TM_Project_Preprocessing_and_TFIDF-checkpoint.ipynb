{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ec58317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import vader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import namedtuple, defaultdict, Counter\n",
    "from math import log10, sqrt\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "271ce30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vader_model = SentimentIntensityAnalyzer()\n",
    "#nlp = spacy.load('en_core_web_sm') # 'en_core_web_sm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7635d228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>there's nothing to tell! he's just some guy i...</td>\n",
       "      <td>monica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c'mon, you're going out with the guy! there's...</td>\n",
       "      <td>joey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>so does he have a hump? a hump and a hairpiece?</td>\n",
       "      <td>chandler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wait, does he eat chalk?</td>\n",
       "      <td>phoebe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>just, 'cause, i don't want her to go through ...</td>\n",
       "      <td>phoebe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52557</th>\n",
       "      <td>no, no, no, no, no!</td>\n",
       "      <td>rachel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52558</th>\n",
       "      <td>no, no, no, no, no!</td>\n",
       "      <td>phoebe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52559</th>\n",
       "      <td>no, no, no, no, no!</td>\n",
       "      <td>joey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52560</th>\n",
       "      <td>no, no, no, no, no!</td>\n",
       "      <td>chandler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52561</th>\n",
       "      <td>no, no, no, no, no!</td>\n",
       "      <td>ross</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52562 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text   Speaker\n",
       "0       there's nothing to tell! he's just some guy i...    monica\n",
       "1       c'mon, you're going out with the guy! there's...      joey\n",
       "2        so does he have a hump? a hump and a hairpiece?  chandler\n",
       "3                               wait, does he eat chalk?    phoebe\n",
       "4       just, 'cause, i don't want her to go through ...    phoebe\n",
       "...                                                  ...       ...\n",
       "52557                                no, no, no, no, no!    rachel\n",
       "52558                                no, no, no, no, no!    phoebe\n",
       "52559                                no, no, no, no, no!      joey\n",
       "52560                                no, no, no, no, no!  chandler\n",
       "52561                                no, no, no, no, no!      ross\n",
       "\n",
       "[52562 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lowercase all text\n",
    "df = pd.read_csv(\"cleaned_data.csv\")\n",
    "df['Text'] = df['Text'].str.lower()\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69f2104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text per person, tokenized and stemmed\n",
    "stemmer = EnglishStemmer()\n",
    "monica_text=[]\n",
    "chandler_text=[]\n",
    "joey_text=[]\n",
    "phoebe_text=[]\n",
    "rachel_text=[]\n",
    "ross_text=[]\n",
    "\n",
    "for i in range(1,len(df)):\n",
    "    text = df[\"Text\"][i]\n",
    "    tokenized_text = nltk.word_tokenize(text)  #tokenize text\n",
    "    #clean tokenized text\n",
    "    table = {ord(char): '' for char in string.punctuation} \n",
    "    cleaned_messy_sentence = []\n",
    "    for messy_word in tokenized_text:   \n",
    "        cleaned_word = messy_word.translate(table) # the translate method allows us to remove all unwanted charachters\n",
    "        cleaned_word_stemmed = stemmer.stem(cleaned_word)\n",
    "        cleaned_messy_sentence.append(cleaned_word_stemmed)\n",
    "    cleaned_sentence = [token for token in cleaned_messy_sentence if token != ''] \n",
    "    \n",
    "    speaker = df[\"Speaker\"][i]\n",
    "    if speaker == \"monica\":\n",
    "        monica_text.append(cleaned_sentence)\n",
    "    elif speaker == \"chandler\":\n",
    "        chandler_text.append(cleaned_sentence)\n",
    "    elif speaker == \"joey\":\n",
    "        joey_text.append(cleaned_sentence)\n",
    "    elif speaker == \"phoebe\":\n",
    "        phoebe_text.append(cleaned_sentence)\n",
    "    elif speaker == \"rachel\":\n",
    "        rachel_text.append(cleaned_sentence)\n",
    "    elif speaker == \"ross\":\n",
    "        ross_text.append(cleaned_sentence)\n",
    "    else: \n",
    "        print(\"SOMETHING WENT WRONG!!!\")\n",
    "        break\n",
    "\n",
    "#print(monica_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec77c06",
   "metadata": {},
   "source": [
    "## Document Frequency and Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f57bfc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_classified = []\n",
    "\n",
    "# For each group, the term frequency.\n",
    "# [0] = monica, [1]=chandler, etc.\n",
    "all_tokens_tf = []\n",
    "# Add all classes into one list\n",
    "all_text_classified.append(monica_text)\n",
    "all_text_classified.append(chandler_text)\n",
    "all_text_classified.append(joey_text)\n",
    "all_text_classified.append(phoebe_text)\n",
    "all_text_classified.append(rachel_text)\n",
    "all_text_classified.append(ross_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0039994",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['monica', 'chandler', 'joey', 'phoebe', 'rachel', 'ross']\n",
    "number_of_speakers = len(class_names)\n",
    "inverted_index = defaultdict(list)\n",
    "tf_matrix = defaultdict(Counter)\n",
    "\n",
    "for index in range(len(all_text_classified)):\n",
    "    group = all_text_classified[index]         # Group = all sentences of a speaker; monica, chandler, etc.\n",
    "    all_tokens = []\n",
    "    for sentence in group:\n",
    "        for token in sentence:\n",
    "            all_tokens.append(token)\n",
    "            \n",
    "    # Document Frequency\n",
    "    term_set = set(all_tokens)\n",
    "    for term in term_set:\n",
    "        inverted_index[term].append(class_names[index])\n",
    "        \n",
    "    # Term Frequency\n",
    "    tf_matrix[class_names[index]] = Counter(all_tokens)   \n",
    "    \n",
    "def tf(term,character):\n",
    "    return float(tf_matrix[character][term])            \n",
    "\n",
    "def df(term):\n",
    "    return float(len(inverted_index[term]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c53023b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monica sweeti 49.0\n",
      "chandler sweeti 3.0\n",
      "joey sweeti 2.0\n",
      "phoebe sweeti 4.0\n",
      "rachel sweeti 14.0\n",
      "ross sweeti 24.0\n",
      "Number of speakers saying the term:  6.0\n"
     ]
    }
   ],
   "source": [
    "# Example TF with text 'term' for a speaker\n",
    "# In order words, how many times does speaker say 'term' (in our data)\n",
    "# NOTE: The speaker does not say the particular term -> (tf = 0, thus tfidf = 0)\n",
    "term = 'sweeti'\n",
    "for speaker in class_names:\n",
    "    print(speaker, term, tf(term, speaker))\n",
    "    \n",
    "# Example DF with text 'term'\n",
    "# Number indicates number of speakers saying the text at least once.\n",
    "# NOTE: All speakers say the particular term -> (idf = 0, thus tfidf = 0)\n",
    "print('Number of speakers saying the term: ', df(term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f963301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDF and TFIDF, with add-one smoothening (taking into account null-values)\n",
    "# NOTE: idf-score = 0 (and thus tfidf when all speakers say the term\n",
    "def idf(term):\n",
    "    return log10((number_of_speakers + 1) / (df(term) + 1))\n",
    "\n",
    "def tfidf(term, speaker):\n",
    "    return tf(term, speaker)*idf(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01f14b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(query_terms, speaker):  # ntn.nnn\n",
    "    score = 0\n",
    "    for term in query_terms:\n",
    "        score = score + tfidf(term,speaker)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fa7ae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_speakers(query):\n",
    "    speakers = []\n",
    "    \n",
    "    for term in query:\n",
    "        all_instances = inverted_index[term]\n",
    "        for character in all_instances:\n",
    "            if character not in speakers:\n",
    "                speakers.append(character)\n",
    "    return speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be79c203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_query(query):\n",
    "    speakers_and_score = []\n",
    "    tokenized_query = nltk.word_tokenize(query)\n",
    "    \n",
    "    # Turn into lowercase\n",
    "    lower_query = []\n",
    "    for token in tokenized_query:\n",
    "        lower_query.append(token.lower())\n",
    "        \n",
    "    #clean query\n",
    "    table = {ord(char): '' for char in string.punctuation} \n",
    "    cleaned_messy_query = []\n",
    "    for messy_word in tokenized_query:   \n",
    "        cleaned_word = messy_word.translate(table) # the translate method allows us to remove all unwanted charachters\n",
    "        cleaned_word_stemmed = stemmer.stem(cleaned_word)\n",
    "        cleaned_messy_query.append(cleaned_word_stemmed)\n",
    "    cleaned_query = [token for token in cleaned_messy_query if token != '']\n",
    "    \n",
    "    \n",
    "    speaker_list = retrieve_speakers(cleaned_query)\n",
    "    \n",
    "    for speaker in speaker_list:\n",
    "        tfidf_score = score(cleaned_query, speaker)\n",
    "        speakers_and_score.append((speaker, tfidf_score))\n",
    "        \n",
    "    ranked = sorted(speakers_and_score, key=lambda x:x[1], reverse=True)\n",
    "    return ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01b345d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chandler', 0.36797678529459443),\n",
       " ('ross', 0.36797678529459443),\n",
       " ('monica', 0.0),\n",
       " ('joey', 0.0),\n",
       " ('phoebe', 0.0),\n",
       " ('rachel', 0.0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score is 0 either when:\n",
    "# 1. The speaker does not say the particular term (tf = 0, thus tfidf = 0)\n",
    "# 2. All speakers say the particular term (idf = 0, thus tfidf = 0)\n",
    "scoring_query('hi bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167f50dd",
   "metadata": {},
   "source": [
    "## Training Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc1bab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration # https://simpletransformers.ai/docs/usage/#configuring-a-simple-transformers-model \n",
    "#model_args = ClassificationArgs()\n",
    "\n",
    "#model_args.overwrite_output_dir=True # overwrite existing saved models in the same directory\n",
    "#model_args.evaluate_during_training=True # to perform evaluation while training the model\n",
    "# (eval data should be passed to the training method)\n",
    "\n",
    "#model_args.num_train_epochs=10 # number of epochs\n",
    "#model_args.train_batch_size=32 # batch size\n",
    "#model_args.learning_rate=4e-6 # learning rate\n",
    "#model_args.max_seq_length=256 # maximum sequence length\n",
    "# Note! Increasing max_seq_len may provide better performance, but training time will increase. \n",
    "# For educational purposes, we set max_seq_len to 256.\n",
    "\n",
    "# Early stopping to combat overfitting: https://simpletransformers.ai/docs/tips-and-tricks/#using-early-stopping\n",
    "#model_args.use_early_stopping=True\n",
    "#model_args.early_stopping_delta=0.01 # \"The improvement over best_eval_loss necessary to count as a better checkpoint\"\n",
    "#model_args.early_stopping_metric='eval_loss'\n",
    "#model_args.early_stopping_metric_minimize=True\n",
    "#model_args.early_stopping_patience=2\n",
    "#model_args.evaluate_during_training_steps=32 # how often you want to run validation in terms of training steps (or batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8caf5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = []\n",
    "#train_target = []\n",
    "#for i in range(100):\n",
    "#    for word in monica_text[i]:\n",
    "#        string = ''\n",
    "#        string.append(word)\n",
    "#        print(string)\n",
    "#        train_data.append(string)\n",
    "#        train_target.append('monica')\n",
    "#    for word in rachel_text[i]:\n",
    "#        string = ''\n",
    "#        string.append(word)\n",
    "#        print(string)\n",
    "#        train_data.append(string)\n",
    "#        train_target.append('rachel')\n",
    "#    for word in phoebe_text[i]:\n",
    "#        string = ''\n",
    "#        string.append(word)\n",
    "#        print(string)\n",
    "#        train_data.append(string)\n",
    "#        train_target.append('phoebe')\n",
    "#train = pd.DataFrame({'text': train_data, 'labels': train_target})\n",
    "\n",
    "#test_data = []\n",
    "#test_target = []\n",
    "#for i in range(100):\n",
    "#    for word in monica_text[i]:\n",
    "#        string = ''\n",
    "#        string.append(word)\n",
    "#        print(string)\n",
    "#        test_data.append(string)\n",
    "#        test_target.append('monica')\n",
    "#    for word in rachel_text[i]:\n",
    "#        string = ''\n",
    "#        string.append(word)\n",
    "#        print(string)\n",
    "#        test_data.append(string)\n",
    "#        test_target.append('rachel')\n",
    "#    for word in phoebe_text[i]:\n",
    "#        string = ''\n",
    "#        string.append(word)\n",
    "#        print(string)\n",
    "#        test_data.append(string)\n",
    "#        test_target.append('phoebe')\n",
    "#test = pd.DataFrame({'text': test_data, 'labels': test_target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "079a5f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking steps per epoch\n",
    "#steps_per_epoch = int(np.ceil(len(train) / float(model_args.train_batch_size)))\n",
    "#print('Each epoch will have {:,} steps.'.format(steps_per_epoch)) # 64 steps = validating 2 times per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46d20696",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train, dev = train_test_split(train, test_size=0.1, random_state=0, \n",
    "#                               stratify=train[['labels']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58e40eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = ClassificationModel('bert', 'bert-base-cased', num_labels=3, args=model_args, use_cuda=False) # CUDA is enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d45e7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_, history = model.train_model(train, eval_df=dev) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7dfc8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation loss\n",
    "#train_loss = history['train_loss']\n",
    "#eval_loss = history['eval_loss']\n",
    "#plt.plot(train_loss, label='Training loss')\n",
    "#plt.plot(eval_loss, label='Evaluation loss')\n",
    "#plt.title('Training and evaluation loss')\n",
    "#plt.legend()\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "#result, model_outputs, wrong_predictions = model.eval_model(dev)\n",
    "#result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5aa6cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted, probabilities = model.predict(test.text.to_list())\n",
    "#test['predicted'] = predicted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
